{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vidhi14/NLP-Basics/blob/main/nlp_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8SFvPyewj0a"
      },
      "source": [
        "Applications of Natural Language Processing:\n",
        "1. Machine Translation\n",
        "2. chatbot\n",
        "3. Speech Recognition \n",
        "4. Sentiment Analysis\n",
        "5. Keyword Search \n",
        "6. information Extraction\n",
        "7. Advertisement Matching "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQlrSNy1wQAk"
      },
      "source": [
        "Natuaral Language Processing consist of two components:\n",
        "1. Natural Language Understanding \n",
        "Mapping i/p to useful representaion \n",
        "analyzing different aspects of the language    \n",
        "**Problems faced:** \n",
        "Lexical ambiguity, syntactic ambiguity and referential ambiguity\n",
        "2. Natural Language Generation  \n",
        "After nlu, \n",
        "text planning, sentence planning and text realization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqcWc-eIme6w",
        "outputId": "309c2d99-d232-403d-ee03-af8b773be264"
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0jHe16CmmEl"
      },
      "source": [
        "Data Processing :(using nltk)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnqXY354mQjs",
        "outputId": "7522891c-629e-4f13-f09e-0c2b00eae2f2"
      },
      "source": [
        "import nltk  \n",
        "nltk.download(\"book\") \n",
        "from nltk.book import*\n",
        "#nltk.download(\"punkt\") \n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "s=\"Hi! I am Vidhi Singh\"\n",
        "tokens = nltk.word_tokenize(s) #for tokenization of a sentence into words\n",
        "print(tokens)\n",
        "tagged=nltk.pos_tag(tokens) #applying part-of-speech tag\n",
        "print(tagged) \n",
        "entities=nltk.chunk.ne_chunk(tagged) #identifying prper names from part-of-speech \n",
        "print(entities)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n",
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
            "['Hi', '!', 'I', 'am', 'Vidhi', 'Singh']\n",
            "[('Hi', 'NN'), ('!', '.'), ('I', 'PRP'), ('am', 'VBP'), ('Vidhi', 'NNP'), ('Singh', 'NNP')]\n",
            "(S (GPE Hi/NN) !/. I/PRP am/VBP (PERSON Vidhi/NNP Singh/NNP))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJA1gweWzdLj",
        "outputId": "2b3202bf-3255-4402-c25c-b11c4b8fda4f"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "import nltk.corpus\n",
        "print(os.listdir(nltk.data.find(\"corpora\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['timit.zip', 'stopwords', 'conll2002', 'gutenberg', 'toolbox', 'brown', 'state_union.zip', 'udhr2', 'ppattach.zip', 'words.zip', 'wordnet', 'abc.zip', 'movie_reviews.zip', 'dependency_treebank.zip', 'cmudict.zip', 'treebank.zip', 'unicode_samples', 'genesis.zip', 'udhr', 'stopwords.zip', 'nps_chat.zip', 'words', 'inaugural.zip', 'unicode_samples.zip', 'conll2000.zip', 'abc', 'city_database.zip', 'udhr.zip', 'webtext.zip', 'names', 'gutenberg.zip', 'brown.zip', 'reuters.zip', 'swadesh', 'udhr2.zip', 'names.zip', 'toolbox.zip', 'wordnet_ic.zip', 'ieer.zip', 'cmudict', 'timit', 'wordnet.zip', 'conll2002.zip', 'city_database', 'ppattach', 'movie_reviews', 'wordnet_ic', 'ieer', 'senseval.zip', 'chat80', 'nps_chat', 'dependency_treebank', 'swadesh.zip', 'state_union', 'treebank', 'genesis', 'senseval', 'panlex_swadesh.zip', 'conll2000', 'inaugural', 'webtext', 'chat80.zip']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT9yknXP3Xq_",
        "outputId": "e26543e9-72d6-454a-e144-e7942497cdf2"
      },
      "source": [
        "from nltk.corpus import webtext \n",
        "webtext.words()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cookie', 'Manager', ':', '\"', 'Don', \"'\", 't', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LYHaQ3l4HSm",
        "outputId": "ff05bba1-3e13-4713-cea1-ef08338913c1"
      },
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjLEFWh14m4H",
        "outputId": "45d07d81-ac10-4e53-c85b-d29bb81e1b59"
      },
      "source": [
        "caesar=nltk.corpus.gutenberg.words('shakespeare-caesar.txt')\n",
        "caesar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'The', 'Tragedie', 'of', 'Julius', 'Caesar', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li5Pvbx26IdX",
        "outputId": "937f7faf-4579-4880-adfe-95f5e53c658c"
      },
      "source": [
        "for word in caesar[:500]:\n",
        "    print(word,end=\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ The Tragedie of Julius Caesar by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Flauius , Murellus , and certaine Commoners ouer the Stage . Flauius . Hence : home you idle Creatures , get you home : Is this a Holiday ? What , know you not ( Being Mechanicall ) you ought not walke Vpon a labouring day , without the signe Of your Profession ? Speake , what Trade art thou ? Car . Why Sir , a Carpenter Mur . Where is thy Leather Apron , and thy Rule ? What dost thou with thy best Apparrell on ? You sir , what Trade are you ? Cobl . Truely Sir , in respect of a fine Workman , I am but as you would say , a Cobler Mur . But what Trade art thou ? Answer me directly Cob . A Trade Sir , that I hope I may vse , with a safe Conscience , which is indeed Sir , a Mender of bad soules Fla . What Trade thou knaue ? Thou naughty knaue , what Trade ? Cobl . Nay I beseech you Sir , be not out with me : yet if you be out Sir , I can mend you Mur . What mean ' st thou by that ? Mend mee , thou sawcy Fellow ? Cob . Why sir , Cobble you Fla . Thou art a Cobler , art thou ? Cob . Truly sir , all that I liue by , is with the Aule : I meddle with no Tradesmans matters , nor womens matters ; but withal I am indeed Sir , a Surgeon to old shooes : when they are in great danger , I recouer them . As proper men as euer trod vpon Neats Leather , haue gone vpon my handy - worke Fla . But wherefore art not in thy Shop to day ? Why do ' st thou leade these men about the streets ? Cob . Truly sir , to weare out their shooes , to get my selfe into more worke . But indeede sir , we make Holyday to see Caesar , and to reioyce in his Triumph Mur . Wherefore reioyce ? What Conquest brings he home ? What Tributaries follow him to Rome , To grace in Captiue bonds his Chariot Wheeles ? You Blockes , you stones , you worse then senslesse things : O you hard hearts , you cruell men of Rome , Knew you not Pompey many a time and oft ? Haue you climb ' d vp to Walles and Battlements , To Towres and Windowes ? Yea , to Chimney tops , Your Infants in your Armes , and there haue sate The liue - long day , with patient expectation , To see great Pompey passe the streets of Rome : And when you saw his Chariot but appeare , Haue you "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtfw4t3c-nYk"
      },
      "source": [
        "**Starting data processing(using nltk)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZOP6q_l9pt2"
      },
      "source": [
        "nlp=\"Natural \\nlanguage processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and \\n analyze large amounts of natural language data. The result is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural-language generation.\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTTli63A9_TX",
        "outputId": "e5a8abb2-2ab5-4605-cdd3-482746967d95"
      },
      "source": [
        "type(nlp)  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc9vcqao-T9-",
        "outputId": "59ca7eca-a643-4b50-bd08-e68dbaa05b08"
      },
      "source": [
        "from nltk.tokenize import* #word.tokenize for tokenization \n",
        "nlp_tokens=word_tokenize(nlp) \n",
        "print(nlp_tokens) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.', 'The', 'result', 'is', 'a', 'computer', 'capable', 'of', 'understanding', 'the', 'contents', 'of', 'documents', ',', 'including', 'the', 'contextual', 'nuances', 'of', 'the', 'language', 'within', 'them', '.', 'The', 'technology', 'can', 'then', 'accurately', 'extract', 'information', 'and', 'insights', 'contained', 'in', 'the', 'documents', 'as', 'well', 'as', 'categorize', 'and', 'organize', 'the', 'documents', 'themselves.Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'speech', 'recognition', ',', 'natural', 'language', 'understanding', ',', 'and', 'natural-language', 'generation', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x03PkOiY_GSu",
        "outputId": "240af7cb-e964-4269-a04e-900bd704d5b2"
      },
      "source": [
        "c=0\n",
        "for word in nlp_tokens[:6]:\n",
        "  print(word)\n",
        "  c=c+1;\n",
        "print(c)\n",
        "#was just trying this\n",
        "print(len(nlp_tokens)) #length of the list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Natural\n",
            "language\n",
            "processing\n",
            "(\n",
            "NLP\n",
            ")\n",
            "6\n",
            "107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPMRvtLj_ZZm",
        "outputId": "c78b397b-5294-4620-fb89-bb0f8f62140d"
      },
      "source": [
        "from nltk.probability import* #for couting frequency\n",
        "freqd=FreqDist() \n",
        "for word in nlp_tokens:\n",
        "  freqd[word.lower()]+=1\n",
        "for i in freqd:\n",
        " print(i+\" \"+(str)(freqd[i]/len(nlp_tokens)))\n",
        " #print(freqd[i]) \n",
        "print(freqd['the'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "natural 0.037383177570093455\n",
            "language 0.056074766355140186\n",
            "processing 0.018691588785046728\n",
            "( 0.009345794392523364\n",
            "nlp 0.009345794392523364\n",
            ") 0.009345794392523364\n",
            "is 0.018691588785046728\n",
            "a 0.018691588785046728\n",
            "subfield 0.009345794392523364\n",
            "of 0.04672897196261682\n",
            "linguistics 0.009345794392523364\n",
            ", 0.056074766355140186\n",
            "computer 0.018691588785046728\n",
            "science 0.009345794392523364\n",
            "and 0.056074766355140186\n",
            "artificial 0.009345794392523364\n",
            "intelligence 0.009345794392523364\n",
            "concerned 0.009345794392523364\n",
            "with 0.009345794392523364\n",
            "the 0.07476635514018691\n",
            "interactions 0.009345794392523364\n",
            "between 0.009345794392523364\n",
            "computers 0.018691588785046728\n",
            "human 0.009345794392523364\n",
            "in 0.028037383177570093\n",
            "particular 0.009345794392523364\n",
            "how 0.009345794392523364\n",
            "to 0.018691588785046728\n",
            "program 0.009345794392523364\n",
            "process 0.009345794392523364\n",
            "analyze 0.009345794392523364\n",
            "large 0.009345794392523364\n",
            "amounts 0.009345794392523364\n",
            "data 0.009345794392523364\n",
            ". 0.028037383177570093\n",
            "result 0.009345794392523364\n",
            "capable 0.009345794392523364\n",
            "understanding 0.018691588785046728\n",
            "contents 0.009345794392523364\n",
            "documents 0.028037383177570093\n",
            "including 0.009345794392523364\n",
            "contextual 0.009345794392523364\n",
            "nuances 0.009345794392523364\n",
            "within 0.009345794392523364\n",
            "them 0.009345794392523364\n",
            "technology 0.009345794392523364\n",
            "can 0.009345794392523364\n",
            "then 0.009345794392523364\n",
            "accurately 0.009345794392523364\n",
            "extract 0.009345794392523364\n",
            "information 0.009345794392523364\n",
            "insights 0.009345794392523364\n",
            "contained 0.009345794392523364\n",
            "as 0.018691588785046728\n",
            "well 0.009345794392523364\n",
            "categorize 0.009345794392523364\n",
            "organize 0.009345794392523364\n",
            "themselves.challenges 0.009345794392523364\n",
            "frequently 0.009345794392523364\n",
            "involve 0.009345794392523364\n",
            "speech 0.009345794392523364\n",
            "recognition 0.009345794392523364\n",
            "natural-language 0.009345794392523364\n",
            "generation 0.009345794392523364\n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg7DyYSiAc9Q",
        "outputId": "22c9c9f9-ef36-440f-c37f-9d92b133e8bd"
      },
      "source": [
        "len(freqd) #number of distinct words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwyvSnywJp3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e633cfd3-0f2e-46d6-b9d7-5d46b66d0cb4"
      },
      "source": [
        " top10=freqd.most_common(2) #for the 2 most frequent word\n",
        " print(top10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 8), ('language', 6)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiyVJbmUf5Vz",
        "outputId": "0e06579b-b482-42ff-decb-bf52285ad6b3"
      },
      "source": [
        "from nltk.tokenize import*\n",
        "nlp_blank=blankline_tokenize(nlp) #for no of pargarphs\n",
        "print(len(nlp_blank))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SXXdVXDhE8a",
        "outputId": "a157fee3-5620-4c9e-abe9-ec8579933849"
      },
      "source": [
        "from nltk.tokenize import*\n",
        "from nltk.util import* \n",
        "s=\"I am having food, in my head given\"\n",
        "token=word_tokenize(s) \n",
        "token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'am', 'having', 'food', ',', 'in', 'my', 'head', 'given']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg6GW1gdqQlt",
        "outputId": "be43e525-7dbb-4c66-8674-fdd84c0120a1"
      },
      "source": [
        "s_bigrams=list(bigrams(token)) \n",
        "print(s_bigrams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('I', 'am'), ('am', 'having'), ('having', 'food'), ('food', ','), (',', 'in'), ('in', 'my'), ('my', 'head'), ('head', 'given')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Btg9pLGpqoCh",
        "outputId": "e80ff220-7a4c-40f8-cc1f-a88c17ca46ac"
      },
      "source": [
        "s_trigrams=list(trigrams(token))\n",
        "print(s_trigrams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('I', 'am', 'having'), ('am', 'having', 'food'), ('having', 'food', ','), ('food', ',', 'in'), (',', 'in', 'my'), ('in', 'my', 'head'), ('my', 'head', 'given')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffwgfhraroDJ",
        "outputId": "790cd6a5-7bfb-424f-d178-5b28bb8cb80b"
      },
      "source": [
        "s_ngrams=list(ngrams(token,4))\n",
        "print(s_ngrams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('I', 'am', 'having', 'food'), ('am', 'having', 'food', ','), ('having', 'food', ',', 'in'), ('food', ',', 'in', 'my'), (',', 'in', 'my', 'head'), ('in', 'my', 'head', 'given')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu2dfXMMr3XL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKiL4xdRsWs5",
        "outputId": "4e4289d7-41a7-437e-b3eb-609e1d92ce41"
      },
      "source": [
        "from nltk.stem import*\n",
        "p=PorterStemmer() #Poreter Stemmer instance created\n",
        "for word in token:\n",
        "  print(word+\":\"+p.stem(word)) \n",
        "pstem=list(set([p.stem(word) for word in token]))  #creating a list #also set or list can be not included\n",
        "print(\"\\n\")\n",
        "print(pstem)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I:I\n",
            "am:am\n",
            "having:have\n",
            "food:food\n",
            ",:,\n",
            "in:in\n",
            "my:my\n",
            "head:head\n",
            "given:given\n",
            "\n",
            "\n",
            "['have', 'I', 'head', ',', 'in', 'my', 'given', 'food', 'am']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFNB8tpzttbQ",
        "outputId": "c98be657-eff6-4d2c-bbc0-5ac55584e458"
      },
      "source": [
        "from nltk.stem import*  \n",
        "a=0\n",
        "l=LancasterStemmer()#Lancaster Stemmer instancec created\n",
        "for word in token: \n",
        "  print(word+\":\"+l.stem(word))   \n",
        "  if((l.stem(word))==\"giv\"):\n",
        "    a+=1\n",
        "print('\\n')\n",
        "print(a)\n",
        "print('\\n')  \n",
        "sb=SnowballStemmer('english') #Snowball Stemmer instance created\n",
        "for word in token: \n",
        "  print(word+\":\"+sb.stem(word)) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I:i\n",
            "am:am\n",
            "having:hav\n",
            "food:food\n",
            ",:,\n",
            "in:in\n",
            "my:my\n",
            "head:head\n",
            "given:giv\n",
            "\n",
            "\n",
            "1\n",
            "\n",
            "\n",
            "I:i\n",
            "am:am\n",
            "having:have\n",
            "food:food\n",
            ",:,\n",
            "in:in\n",
            "my:my\n",
            "head:head\n",
            "given:given\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfR9DAIpubGQ"
      },
      "source": [
        "Stemming: cut the suffix and prefix , may not be absolutely coorect \n",
        "Lemmatization: maps the word to it's morphological root word \n",
        "VARD, a sentiment analysis software does not do stemming and lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3XRI0bwux6w",
        "outputId": "2b2553df-48d6-4360-a5f0-f7336235965c"
      },
      "source": [
        "from nltk.stem import wordnet #importing the word set i.e. dictionary of root words\n",
        "from nltk.stem import WordNetLemmatizer #importing the function\n",
        "lem=WordNetLemmatizer() \n",
        "l=list([lem.lemmatize(word) for word in token])  \n",
        "print(l)  #since we have not assigned parts of speech, it considers all to be noun"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'am', 'having', 'food', ',', 'in', 'my', 'head', 'given']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBIeMCTvulzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec8af6c-f05a-4c8e-8e65-4d102f011279"
      },
      "source": [
        "#stop words: helpful in english language but not for natural language \n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))  \n",
        "print(len(stopwords.words('english')))   \n",
        "#removing stopwords from the given data\n",
        "sw=list(stopwords.words('english'))\n",
        "ars=[word for word in token if not word in sw] \n",
        "print(ars)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "179\n",
            "['I', 'food', ',', 'head', 'given']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTmrpbdqwzov",
        "outputId": "19009617-a47b-470e-8a73-d5955c79c31c"
      },
      "source": [
        "#after removing punctuation marks\n",
        "import re \n",
        "punctuation=re.compile(r'[-,:;?/!()|0-9]') \n",
        "after_removing_punctuation=[]\n",
        "for word in token: \n",
        "  w=punctuation.sub(\"\",word) #remove everything except space and the word\n",
        "  if(len(w)>0):\n",
        "    after_removing_punctuation.append(w) \n",
        "print(after_removing_punctuation)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'am', 'having', 'food', 'in', 'my', 'head', 'given']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgBktBX56mqB",
        "outputId": "75781c60-4589-4cd5-9d82-4c9643424121"
      },
      "source": [
        "#Just experimenting\n",
        "import re\n",
        "\n",
        "line = \"dance more\" \n",
        "b=line.split() \n",
        "print(b)\n",
        "result = re.match(r\"[^\\d+]\", line)\n",
        "print(result)     # Prints out 'dance'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dance', 'more']\n",
            "<re.Match object; span=(0, 1), match='d'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFqU11M08Tec",
        "outputId": "b7b5011d-f3aa-4175-c09c-b53dbf669903"
      },
      "source": [
        "#pos tagging Part of Speech tagging\n",
        "pos=(list([nltk.pos_tag(word) for word in token])) \n",
        "print(pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[('I', 'PRP')], [('a', 'DT'), ('m', 'NN')], [('h', 'VB'), ('a', 'DT'), ('v', 'NN'), ('i', 'NN'), ('n', 'VBP'), ('g', 'NN')], [('f', 'JJ'), ('o', 'NN'), ('o', 'NN'), ('d', 'NN')], [(',', ',')], [('i', 'NN'), ('n', 'VBP')], [('m', 'NN'), ('y', 'NN')], [('h', 'NN'), ('e', 'VBZ'), ('a', 'DT'), ('d', 'NN')], [('g', 'NN'), ('i', 'NN'), ('v', 'VBP'), ('e', 'NN'), ('n', 'NN')]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Uf51093A40g",
        "outputId": "476b732b-0af8-45a2-9095-edcb30cdb1ec"
      },
      "source": [
        "lem=WordNetLemmatizer() \n",
        "lem2=list([lem.lemmatize(word) for word in after_removing_punctuation]) \n",
        "print(lem2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'am', 'having', 'food', 'in', 'my', 'head', 'given']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8oekeZGB7F3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JilSmGFeDCiN"
      },
      "source": [
        "Name Entity REcognition: for phrases , proper nouns, organizations ,etc.  \n",
        "Syntax tree: representaion of syntactic structure of sentences or strings"
      ]
    }
  ]
}